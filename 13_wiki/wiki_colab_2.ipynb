{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlbVItZt6ivt"
   },
   "source": [
    "<div style=\"border:solid blue 2px; padding: 20px\">\n",
    "<font color='blue'>Привет, Миша!</font>\n",
    "\n",
    "Твоё решение проверил <font color='blue'>Александр Цымбалов</font>\n",
    "\n",
    "Если у тебя будут какие-то <font color='blue'>вопросы</font>, то обязательно их <font color='blue'>задавай</font>\n",
    "\n",
    "Я буду обращаться к тебе на \"ты\", ведь все мы здесь студенты)\n",
    "Но если тебе некомфортно, то дай знать, пожалуйста\n",
    "<br><br>\n",
    "Мои комментарии будут в отдельных ячейках <font color='blue'>Markdown</font> с заголовком «Комментарий ревьюера».\n",
    "\n",
    "**Пожалуйста, не перемещай, не изменяй и не удаляй их**. \n",
    "Я буду использовать цветовую разметку:\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Комментарий ревьюера ❌:</b> Замечания, которые необходимо обработать, иначе я не смогу принять проект </div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Комментарий ревьюера ⚠️:</b> Небольшие замечания или вопросы по пониманию темы. Если есть желание и время, то у тебя есть возможность сделать проект ещё лучше </div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Комментарий ревьюера ✔️:</b> Хвалебные отзывы или другого рода комментарии)</div>\n",
    "\n",
    "**Если ты что-то меняешь в проекте или отвечаешь на мои комментарии — пиши об этом.**\n",
    "\n",
    "Мне будет <font color='blue'> легче </font> отследить изменения, если ты выделишь свои комментарии:\n",
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Лучше в таком формате, но как тебе <font color='blue'> удобно </font></div></font>\n",
    "\n",
    "Версии комментариев я буду нумеровать\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lq-ysQT6ivy"
   },
   "source": [
    "<div style=\"border:solid blue 2px; padding: 20px\">\n",
    "<font color='blue'>Итоги 1 ревью</font>\n",
    "\n",
    "Порог в работе не пройден, но основные действия по работе с BERT и обучение моделей произведены в большей части корректно. Что я могу предложить, чтобы увеличить показатели по метрике:\n",
    "    <ul>\n",
    "        <li>Использовать для обучения Google Colab</li>\n",
    "        <li>Увеличить выборку (см про Google colab)</li>\n",
    "        <b><li>Обязательно очистить текст перед работой (можно без лемматизации)</li></b>\n",
    "        <li>Подобрать гиперпараметры для моделей</li>\n",
    "        <li>Не масштабировать эмбеддинги</li>\n",
    "    </ul>\n",
    "Также хочу заметить, что TF-IDF/word2vec + модели классического ML тоже неплохо справляются с задачей. \n",
    "    <br><br>\n",
    "    \n",
    "<font color='blue'>Материалы, которые помогут разобраться:</font>\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/bert.html \\\n",
    "https://t.me/renat_alimbekov \\\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - Про LSTM \\\n",
    "https://web.stanford.edu/~jurafsky/slp3/10.pdf - про энкодер-декодер модели, этеншены\\\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html - официальный гайд\n",
    "по трансформеру от создателей pytorch\\\n",
    "https://transformer.huggingface.co/ - поболтать с трансформером \\\n",
    "Библиотеки: allennlp, fairseq, transformers, tensorflow-text — множествореализованных\n",
    "методов для трансформеров методов NLP \\\n",
    "Word2Vec https://radimrehurek.com/gensim/models/word2vec.html \n",
    "\n",
    "<font color='green'>Пример BERT с GPU:\n",
    "```python\n",
    "%%time\n",
    "from tqdm import notebook\n",
    "batch_size = 2 # для примера возьмем такой батч, где будет всего две строки датасета\n",
    "embeddings = [] \n",
    "for i in notebook.tqdm(range(input_ids.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(input_ids[batch_size*i:batch_size*(i+1)]).cuda() # закидываем тензор на GPU\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.cuda()\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy()) # перевод обратно на проц, чтобы в нумпай кинуть\n",
    "        del batch\n",
    "        del attention_mask_batch\n",
    "        del batch_embeddings\n",
    "        \n",
    "features = np.concatenate(embeddings) \n",
    "```\n",
    "Можно сделать предварительную проверку на наличие GPU.\\\n",
    "Например, так: ```device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")```\\\n",
    "Тогда вместо .cuda() нужно писать .to(device)</font><br><br>\n",
    "Жду на ревью. Удачи!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Uydan0-P6ivz",
    "tags": [
     "045a5103-bb0f-4fb7-835a-238e053b2157"
    ]
   },
   "source": [
    "# Проект для «Викишоп» c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1x3xTAz6iv0"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>1 Комментарий ревьюера ✔️:</b> Здорово, что ты взял BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmiV9mw16iv0"
   },
   "source": [
    "### Описание проекта\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "Постройте модель со значением метрики качества F1 не меньше 0.75.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyFaqE7Q6iv1"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>1 Комментарий ревьюера ✔️:</b> Хорошая практика вставлять описание работы для того, чтобы видеть задачу перед глазами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "MsGwFrKm8DMd"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "eIDdfoUk6iv1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import spacy\n",
    "from tqdm import notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os\n",
    "import re\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqk2LucTDPb8"
   },
   "source": [
    "**Доступность карты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SDuW1M8DDw3",
    "outputId": "ec105a65-212a-4911-9fb4-a92db72526b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Проверяю, доступна ли видеокарта\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D82db_LZ6iv3",
    "outputId": "9eccda2a-9be0-47aa-97a0-638e3a31341b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel,\n",
    "                                                    transformers.DistilBertTokenizer,\n",
    "                                                    'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "xhVF2hgu6iv4"
   },
   "outputs": [],
   "source": [
    "# def get_data(filename):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Reads the data from the given path.\n",
    "#     \"\"\"\n",
    "\n",
    "#     pth1 = f'/datasets/{filename}'\n",
    "#     pth2 = f'data/{filename}'\n",
    "\n",
    "#     if os.path.exists(pth1):\n",
    "#         df = pd.read_csv(pth1)\n",
    "#         print(f'Reading {pth1}...')\n",
    "#     elif os.path.exists(pth2):\n",
    "#         df = pd.read_csv(pth2)\n",
    "#         print(f'Reading {pth2}...')\n",
    "#     else:\n",
    "#         print('Check the file path')\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58OAZ5_W6iv5"
   },
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "15xbMwYiUkfF"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6sGNgmPt6iv5",
    "outputId": "10f529a0-da0b-48f7-ce89-772af6832795"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e14009ad-4d6b-4763-b044-6358f6091719\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" March 2006 (UTC)\\n\\nActually, I gave two, bo...</td>\n",
       "      <td>0</td>\n",
       "      <td>march utc actually i gave two both of which ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To whom it may concern \\nThis is a talk page, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>to whom it may concern this is a talk page not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why is this front page news  \\n\\njust asking, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>why is this front page news just asking nobody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>==\\nI IDOLIZE YOU MAN!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>i idolize you man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\\n\\n Some baklava for you! \\n\\n  A Gift, in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>some baklava for you a gift in the hopes that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e14009ad-4d6b-4763-b044-6358f6091719')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e14009ad-4d6b-4763-b044-6358f6091719 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e14009ad-4d6b-4763-b044-6358f6091719');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  \" March 2006 (UTC)\\n\\nActually, I gave two, bo...      0   \n",
       "1  To whom it may concern \\nThis is a talk page, ...      0   \n",
       "2  why is this front page news  \\n\\njust asking, ...      0   \n",
       "3                           ==\\nI IDOLIZE YOU MAN!!!      0   \n",
       "4  \"\\n\\n Some baklava for you! \\n\\n  A Gift, in t...      0   \n",
       "\n",
       "                                          clear_text  \n",
       "0  march utc actually i gave two both of which ar...  \n",
       "1  to whom it may concern this is a talk page not...  \n",
       "2  why is this front page news just asking nobody...  \n",
       "3                                  i idolize you man  \n",
       "4  some baklava for you a gift in the hopes that ...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = get_data('toxic_comments.csv')\n",
    "# Эмбеддинги буду получать для подвыборки (иначе слишком долго)\n",
    "df = df.sample(50000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMEGPQAM6iv6",
    "outputId": "f89f044d-7e1a-4fba-d105-c5ccb033e97d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RV5qgnFeB4Sn",
    "outputId": "a7749764-67ff-4c78-a79a-7c7f34304316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.89814\n",
       "1    0.10186\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagGxlcpCIYk"
   },
   "source": [
    "> В обучающей выборке существенный дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hQzbmTVEJO5"
   },
   "source": [
    "**Очистка текста**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSNWnbnuVdGu"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Комментарий студента:</b>\n",
    "Очистил текст\n",
    "</font></div></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "g8FMgeAZONzu"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "42XnEKZ2A2uo"
   },
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    '''\n",
    "    Text cleaning\n",
    "    '''\n",
    "    \n",
    "    text_patterned = re.sub(r'[^a-zA-Z]', ' ', text.lower())\n",
    "    text_splitted = text_patterned.split()\n",
    "    text_spaced = ' '.join(text_splitted)\n",
    "    \n",
    "    return text_spaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "7dlkoALmOEcg"
   },
   "outputs": [],
   "source": [
    "# def lemmatize(text):\n",
    "#     global nlp\n",
    "#     doc = nlp(text)\n",
    "#     return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmsX35fjBB5s",
    "outputId": "56b5c1e9-fe90-4ea8-855d-abb7cd459755"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        march utc actually i gave two both of which ar...\n",
       "1        to whom it may concern this is a talk page not...\n",
       "2        why is this front page news just asking nobody...\n",
       "3                                        i idolize you man\n",
       "4        some baklava for you a gift in the hopes that ...\n",
       "                               ...                        \n",
       "49995    a little much is it really necessary to mentio...\n",
       "49996    i have to disagree i will not be changing my w...\n",
       "49997    could the name have something to do with the p...\n",
       "49998    the perquot were used as slaves or traded as s...\n",
       "49999    list of managers hey thanks for the warning i ...\n",
       "Name: clear_text, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clear_text'] = df['text'].apply(clear_text)\n",
    "\n",
    "df['clear_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "xHGi6c4xOblG"
   },
   "outputs": [],
   "source": [
    "# df['clear_text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPUmacMt6iv6"
   },
   "source": [
    "### Токенизация и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfzwRK0W6iv6",
    "outputId": "17b7101a-3d36-4dc6-d9d6-f2a41df64a2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tokenized = df['clear_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1A2AsJB6iv7"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b5H9K-96iv7"
   },
   "outputs": [],
   "source": [
    "# Извлечение эмбеддингов\n",
    "batch_size = 200\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "    \n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "    \n",
    "    del batch\n",
    "    del attention_mask_batch\n",
    "    del batch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZxj-syl6iv7"
   },
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAH2JlOyRdRx"
   },
   "outputs": [],
   "source": [
    "# # Сохраняю эмбеддинги для запуска проекта локально\n",
    "# with open('embeddings_50000.npy', 'wb') as f:\n",
    "#     np.save(f, embeddings)\n",
    "# with open('embeddings_50000.npy', 'rb') as f:\n",
    "#     a = np.load(f)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRCQBZ-y6iv8"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>1 Комментарий ревьюера ✔️:</b> Правильная работа с BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woWxcpkS6iv8"
   },
   "outputs": [],
   "source": [
    "# features = np.concatenate(embeddings)\n",
    "targets = df['toxic']\n",
    "\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, targets,\n",
    "                                                                            test_size=.25,\n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2bHUtB56iv8"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>1 Комментарий ревьюера ⚠️:</b> На курсе принято делить выборку в пропорциях 75% train 25% test. Кажется, что 25% весьма существенны для обучения</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCPB-JHrEdvp"
   },
   "source": [
    "**Исправил**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4tIU7_Q6iv8"
   },
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1QuZ5Ma6iv9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC5u6qKh6iv9"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>1 Комментарий ревьюера ⚠️:</b> Поиск дисбаланса можно было произвести при первоначальном анализе данных</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-m9OthxEy24"
   },
   "source": [
    "**Перенес**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXCxfQ_IMsW4"
   },
   "source": [
    "### catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCqMRmK7MxPC"
   },
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szHApH7GMzO1"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "HUMfSx6bM3U0",
    "outputId": "c3eab6e7-48bc-431f-b249-0c3ab5e52234"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-f0c1f86435d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12345\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clear_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5007\u001b[0m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   5008\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5009\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0m\u001b[1;32m   5010\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2268\u001b[0m             \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_snapshot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_snapshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m             \u001b[0msnapshot_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msnapshot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msnapshot_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m         )\n\u001b[1;32m   2272\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[1;32m   2149\u001b[0m                                        \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m                                        baseline, column_description)\n\u001b[0m\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_build_train_pool\u001b[0;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n\u001b[0;32m-> 1431\u001b[0;31m                           group_weight=group_weight, subgroup_id=subgroup_id, pairs_weight=pairs_weight, baseline=baseline)\n\u001b[0m\u001b[1;32m   1432\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                 self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0;32m--> 791\u001b[0;31m                            group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_string_feature_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_features_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_string_feature_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_string_feature_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_get_features_indices\u001b[0;34m(features, feature_names)\u001b[0m\n\u001b[1;32m    267\u001b[0m         return [\n\u001b[1;32m    268\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         ]\n\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    267\u001b[0m         return [\n\u001b[1;32m    268\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         ]\n\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'clear_text' is not in list"
     ]
    }
   ],
   "source": [
    "cat = CatBoostClassifier(random_state=12345, task_type='GPU')\n",
    "cat.fit(pd.DataFrame(train_features), train_target, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgZyFUBWNHaW",
    "outputId": "9f39c769-a062-4a9d-b553-179a0b55de23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7069681587448085\n"
     ]
    }
   ],
   "source": [
    "cat_pred = cat.predict(pd.DataFrame(test_features))\n",
    "print(f1_score(cat_pred, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgbCt3hn6iv9"
   },
   "source": [
    "#### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sTafY8d6iv9",
    "outputId": "f691e50f-5c11-42f4-f043-18680b3ecb38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train F1: 0.69\n",
      "test F1: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "lg_model = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "lg_model.fit(train_features, train_target)\n",
    "\n",
    "train_predict = lg_model.predict(train_features)\n",
    "print(f'train F1: {f1_score(train_target, train_predict):.2f}')\n",
    "\n",
    "test_predict = lg_model.predict(test_features)\n",
    "print(f'test F1: {f1_score(test_target, test_predict):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACafFxox6iv9"
   },
   "outputs": [],
   "source": [
    "lg_CV_model = LogisticRegressionCV(cv=50, scoring='f1', max_iter=500, class_weight='balanced')\n",
    "lg_CV_model.fit(train_features, train_target)\n",
    "\n",
    "train_predict = lg_CV_model.predict(train_features)\n",
    "print(f'train F1: {f1_score(train_target, train_predict):.2f}')\n",
    "\n",
    "test_predict = lg_CV_model.predict(test_features)\n",
    "print(f'test F1: {f1_score(test_target, test_predict):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3ctLEo46iv9"
   },
   "source": [
    "> Логистическая регрессия дает результат для метрики F1 на 0.1 ниже требуемого порогового"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yvSRdGk6iv-"
   },
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_G7uOet6iv-"
   },
   "outputs": [],
   "source": [
    "model = LGBMClassifier(class_weight='balanced', n_estimators=500)\n",
    "# Оценка качества модели\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, train_features, train_target, scoring='f1', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print(f'F1: {np.mean(n_scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5YrBcEP6iv-"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Комментарий студента</b>\n",
    "    \n",
    "В обсуждении прочитал, что требуется использовать пайплан, так как в противном случае на тренировочных данных будет пременяться векторизатор, обученный на всех исходных данных (test + train). Я так и не понял, как это реализовать, но пайплайны построить попробовал:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ed7f4R6iv-"
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>1 Комментарий ревьюера ❌:</b> Я не думаю, что уместно для эмбеддингов BERT использовать масштабирование, т.к. они уже сами по себе в некотором роде отмасштабированы. Не стоит здесь использовать StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr_qtHJg6iv-"
   },
   "outputs": [],
   "source": [
    "# В обсуждении прочитал, что требуется использовать пайплан, так как\n",
    "# векторизатор обучается на всей выборке\n",
    "pipeline = Pipeline([\n",
    "    ('logreg', LogisticRegression(max_iter=500, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipeline.fit(train_features, train_target)\n",
    "pipeline_predictions = pipeline.predict(test_features)\n",
    "\n",
    "# Score pipeline on testing data\n",
    "print(f'test F1: {f1_score(test_target, pipeline_predictions):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOppKXwtTdag",
    "outputId": "f82b9d78-14de-49ce-de37-bc2b66debf51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-04, 1.e-02, 1.e+00, 1.e+02, 1.e+04])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.logspace(-4, 4, 5)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUfXwF3O6iv-"
   },
   "outputs": [],
   "source": [
    "# Create the pipeline object\n",
    "# Note this is identical to the code above\n",
    "polynomial_pipeline = Pipeline([\n",
    "    ('logreg', LogisticRegressionCV(cv=20, scoring='f1', max_iter=500, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Create new parameter dictionary\n",
    "grid_params = {\n",
    "    'logreg__penalty': ('l1', 'l2'),\n",
    "    'C': np.logspace(-4, 4, 5),\n",
    "}\n",
    "\n",
    "# Instantiate new gridsearch object\n",
    "gs_2 = GridSearchCV(polynomial_pipeline, grid_params)\n",
    "\n",
    "# Fit model to our training data\n",
    "gs_2.fit(train_features, train_target)\n",
    "gs2_predictions = gs_2.predict(test_features)\n",
    "\n",
    "# Score pipeline on testing data\n",
    "print(f'test F1: {f1_score(test_target, gs2_predictions):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Mxkq8KD6iv_"
   },
   "source": [
    "> Все другие реализованные методы даюр результат хуже, чем логистическая регрессия. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUt6adPQ6iv_"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Комментарий студента</b>\n",
    "    \n",
    "Я явно упускаю что-то важное, но попробую разобраться с твоими подсказками.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdpIFCy86iv_"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>1 Комментарий ревьюера ⚠️:</b> См. итоги 1 ревью"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3493,
    "start_time": "2022-06-10T09:09:50.138Z"
   },
   {
    "duration": 45,
    "start_time": "2022-06-10T09:09:53.633Z"
   },
   {
    "duration": 2685,
    "start_time": "2022-06-10T09:09:53.680Z"
   },
   {
    "duration": 18942,
    "start_time": "2022-06-10T09:09:56.368Z"
   },
   {
    "duration": 468647,
    "start_time": "2022-06-10T09:10:15.312Z"
   },
   {
    "duration": 61,
    "start_time": "2022-06-10T09:18:19.116Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-10T09:18:56.192Z"
   },
   {
    "duration": 53,
    "start_time": "2022-06-10T09:19:28.961Z"
   },
   {
    "duration": 384,
    "start_time": "2022-06-10T09:19:30.073Z"
   },
   {
    "duration": 2551,
    "start_time": "2022-06-10T09:19:31.330Z"
   },
   {
    "duration": 104093,
    "start_time": "2022-06-10T09:19:36.116Z"
   },
   {
    "duration": 6,
    "start_time": "2022-06-10T09:21:30.120Z"
   },
   {
    "duration": 7,
    "start_time": "2022-06-10T09:23:44.512Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-10T09:24:18.574Z"
   },
   {
    "duration": 1486,
    "start_time": "2022-06-10T09:24:26.334Z"
   }
  ],
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook (5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4debb497d9efd2ed82dd6f02c53bc2b8d2bebf2a671e267e579eb08f7159d111"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
